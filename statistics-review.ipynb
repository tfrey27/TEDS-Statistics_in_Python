{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Statistics Fundamentals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "## Learning Objectives\n",
    "- **Linear algebra:** Dot products, matrix multiplications, and vector norms by hand and using NumPy.\n",
    "- **Summary statistics:** Using NumPy and Pandas: mean, median, mode, max, min, quartile, inter-quartile range, variance, standard deviation, and correlation.\n",
    "- **Discover trends:** Using basic summary statistics and viz.\n",
    "- **Bias/variance tradeoff:** Describe the bias and variance of statistical estimators.\n",
    "- **Identify a normal distribution** within a data set using summary statistics and data visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Where Are We in the Data Science Workflow?](#where-are-we-in-the-data-science-workflow)\n",
    "- [Linear Algebra Review](#linear-algebra-review)\n",
    "    - [Scalars, Vectors, and Matrices](#scalars-vectors-and-matrices)\n",
    "\t- [Basic Matrix Algebra](#basic-matrix-algebra)\n",
    "\t- [Dot Product](#dot-product)\n",
    "\t- [Matrix Multiplication](#matrix-multiplication)\n",
    "\t- [N-Dimensional Space](#n-dimensional-space)\n",
    "\t- [Vector Norm](#vector-norm)\n",
    "- [Linear Algebra Applications to Machine Learning](#linear-algebra-applications-to-machine-learning)\n",
    "\t- [Distance Between Actual Values and Predicted Values](#distance-between-actual-values-and-predicted-values)\n",
    "\t- [Mean Squared Error](#mean-squared-error)\n",
    "\t- [Least Squares](#least-squares)\n",
    "- LESSON BREAK\n",
    "- [Code-Along: Examining the Titanic Data Set](#codealong-examining-the-titanic-dataset)\n",
    "- [Descriptive Statistics Fundamentals](#descriptive-statistics-fundamentals)\n",
    "\t- [Measures of Central Tendency](#measures-of-central-tendency)\n",
    "\t- [Math Review](#math-review)\n",
    "\t- [Measures of Dispersion: Standard Deviation and Variance](#measures-of-dispersion-standard-deviation-and-variance)\n",
    "- [The Normal Distribution](#the-normal-distribution)\n",
    "\t- [What is the Normal Distribution?](#what-is-the-normal-distribution)\n",
    "\t- [Skewness](#skewness)\n",
    "\t- [Kurtosis](#kurtosis)\n",
    "- [Determining the Distribution of Your Data](#determining-the-distribution-of-your-data)\n",
    "\t- [Exercise](#exercise)\n",
    "- [Lesson Review](#topic-review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# This makes sure that graphs render in your notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-algebra-review\"></a>\n",
    "## Linear Algebra Review\n",
    "---\n",
    "**Objective:** Compute dot products, matrix multiplications, and vector norms by hand and using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-linear-algebra\"></a>\n",
    "## Why Use Linear Algebra in Data Science?\n",
    "\n",
    "Linear models are efficient and well understood. They can often closely approximate nonlinear solutions, and they scale to high dimensions without difficulty.\n",
    "\n",
    "Because linear models are so prevalent, we should study them in-depth. **Linear models are all based on linear algebra**, so we should know that too.\n",
    "\n",
    "Actually, most advanced models rely on linear algebra too. Principal components analysis can really only be explained in terms of linear algebra. Advanced models such as neural nets and support vector machines rely on linear algebra to work at all. Modern GPUs are basically linear algebra machines and can therefore be used to speed up complicated models.\n",
    "\n",
    "Furthermore, even the complicated models rely on the basic models, which in turn rely heavily on linear algebra. There is no avoiding this topic. _**Eat your vegetables.**_\n",
    "\n",
    "Although we do not have time in this course to comprehensively discuss linear algebra, we highly recommend you become fluent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scalars-vectors-and-matrices\"></a>\n",
    "## Vocab: Scalars, Vectors, and Matrices\n",
    "\n",
    "A **scalar** is a single number. Here, symbols that are lowercase single letters refer to scalars. For example, the symbols $a$ and $v$ are scalars that might refer to arbitrary numbers such as $5.328$ or $7$. An example scalar would be:\n",
    "\n",
    "$$a$$\n",
    "\n",
    "A **vector** is an ordered sequence of numbers. Here, symbols that are lowercase single letters with an arrow — such as $\\vec{u}$ — refer to vectors. (Or, often written as $\\mathbf{u}$). An example vector would be:\n",
    "\n",
    "$$\\vec{u} = \\left[ \\begin{array}{c}\n",
    "1&3&7\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "_Note:_ It's usually easy to consider vectors as either a $1 \\times n$ or $n \\times 1$ \"row\" or \"column\" vector, where convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a vector using np.array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $m$ x $n$ **matrix** is a rectangular array of numbers with $m$ rows and $n$ columns. Each number in the matrix is an entry. Entries can be denoted $a_{ij}$, where $i$ denotes the row number and $j$ denotes the column number. Note that, because each entry $a_{ij}$ is a lowercase single letter, a matrix is an array of scalars:\n",
    "\n",
    "$$\\mathbf{A}= \\left[ \\begin{array}{c}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n}  \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n}  \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{array} \\right]$$\n",
    "\n",
    "Matrices are referred to using bold uppercase letters, such as $\\mathbf{A}$. A bold font face is used to distinguish matrices from sets. (Sometimes, not always)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix using np.array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in Python, a matrix is just a list of lists converted to a numpy array! The outermost list is a list of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"basic-matrix-algebra\"></a>\n",
    "### Basic Matrix Algebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition and Subtraction\n",
    "Vector **addition** is straightforward. If two vectors are of equal dimensions (The vectors are shown here as column vectors for convenience only):\n",
    "\n",
    "$\\vec{v} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right],  \\vec{w} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "v = np.array([1, 3, 7])\n",
    "w = np.array([1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{v} + \\vec{w} =\n",
    "\\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right] + \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right] = \n",
    "\\left[ \\begin{array}{c}\n",
    "1+1 \\\\\n",
    "3+0 \\\\\n",
    "7+1\n",
    "\\end{array} \\right] = \n",
    "\\left[ \\begin{array}{c}\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "8\n",
    "\\end{array} \\right]\n",
    "$\n",
    "\n",
    "(Subtraction is similar.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add the vectors together with +."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar Multiplication\n",
    "We scale a vector with **scalar multiplication**, multiplying a vector by a scalar (single quantity):\n",
    "\n",
    "$ 2 \\cdot \\vec{v} = 2\\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right] = \n",
    " \\left[ \\begin{array}{c}\n",
    "2 \\cdot 1 \\\\\n",
    "2 \\cdot 3 \\\\\n",
    "2 \\cdot 7\n",
    "\\end{array} \\right] = \n",
    " \\left[ \\begin{array}{c}\n",
    "2 \\\\\n",
    "6 \\\\\n",
    "14\n",
    "\\end{array} \\right]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Multiply v by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vector-norm\"></a>\n",
    "### Vector Norm \n",
    "\n",
    "The **magnitude** of a vector, $\\vec{v} \\in \\mathbb{R}^{n}$, can be interpreted as its length in $n$-dimensional space. Therefore it is calculable via the Euclidean distance from the origin:\n",
    "\n",
    "$\\vec{v} = \\left[ \\begin{array}{c}\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "\\vdots \\\\\n",
    "v_{n}\n",
    "\\end{array} \\right]$\n",
    "\n",
    "then $\\| \\vec{v} \\| = \\sqrt{v_{1}^{2} + v_{2}^{2} + ... + v_{n}^{2}} = \\sqrt{\\vec{v}^T\\vec{v}}$\n",
    "\n",
    "E.g. if $\\vec{v} = \n",
    "\\left[ \\begin{array}{c}\n",
    "3 \\\\\n",
    "4\n",
    "\\end{array} \\right]$, then $\\| \\vec{v} \\| = \\sqrt{3^{2} + 4^{2}} = 5$\n",
    "\n",
    "This is also called the vector **norm**. You will often see this used in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the norm of the vector x with np.linalg.norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dot-product\"></a>\n",
    "### Dot Product\n",
    "The **dot product** of two _n_-dimensional vectors is:\n",
    "\n",
    "$ \\vec{v} \\cdot \\vec{w} =\\sum _{i=1}^{n}v_{i}w_{i}=v_{1}w_{1}+v_{2}w_{2}+\\cdots +v_{n}w_{n} $\n",
    "\n",
    "So, if:\n",
    "\n",
    "$\\vec{v} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "7\n",
    "\\end{array} \\right], \\vec{w} = \\left[ \\begin{array}{c}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} \\right]$\n",
    "\n",
    "$ \\vec{v} \\cdot \\vec{w} = 1 \\cdot 1 + 3 \\cdot 0 + 7 \\cdot 1 = 8 $\n",
    "\n",
    "_Note:_ When considering vectors as \"column vectors\", you will often see a dot product written as $\\mathbf{v}^T\\mathbf{w}$. In more pure-math based literature, you might even see $\\langle v, w \\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dot product of v and w using np.dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Where do we use the dot product?_\n",
    "\n",
    "1. It provides a quick way of computing the magnitude of a vector $v$:\n",
    "\n",
    "$$|v| = \\sqrt{v \\cdot v}.$$\n",
    "\n",
    "2. It is related to the angle $\\theta$ between two vectors $v$ and $w$:\n",
    "\n",
    "$$v \\cdot w = |v| |w| \\cos{\\theta}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"matrix-multiplication\"></a>\n",
    "### Matrix Multiplication\n",
    "**Matrix multiplication**, $\\mathbf{AB}$, is valid when the left matrix has the same number of columns as the right matrix has rows. Each entry is the dot product of corresponding row and column vectors.\n",
    "\n",
    "![](assets/images/matrix-multiply-a.gif)\n",
    "(Image: mathisfun.com!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product illustrated above is: $1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11 = 58$. Can you compute the rest of the dot products by hand?\n",
    "\n",
    "If the product is the $2$ x $2$ matrix $\\mathbf{C}$, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the two above matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_Why is matrix multiplication important?_\n",
    "\n",
    "In data science, we work with matrices of data! If we want to apply an operation to the matrix, it is nearly always performed using some sort of matrix operation.\n",
    "\n",
    "For example, suppose we are using linear regression to predict house prices. We guess that price is related to square footage and lot size as follows:\n",
    "\n",
    "$$\\mathbf{Y} = \\hat{\\text{price}} = 10 \\cdot \\text{sqft} + 5 \\cdot \\text{lotsize} + 2000.$$\n",
    "\n",
    "We want to generate many predictions at once given many houses. How might we do this using matrix multiplication? We'll use the following formula, where $\\mathbf{X}$ is our data and $\\beta$ is a column vector of our coefficients:\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\beta.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                      sqft   lotsize                    \n",
    "houses = np.array([[1, 5000, 7000],\n",
    "                   [1, 2000, 2500],\n",
    "                   [1, 1000, 1000]])\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"n-dimensional-space\"></a>\n",
    "### N-Dimensional Space\n",
    "\n",
    "We often refer to vectors as elements of an $n$-dimensional space. The symbol $\\mathbb{R}$ refers to the set of all real numbers (written in uppercase \"blackboard bold\" font). Because this contains all reals, $3$ and $\\pi$ are **contained in** $\\mathbb{R}$. We often write this symbolically as $3 \\in \\mathbb{R}$ and $\\pi \\in \\mathbb{R}$.\n",
    "\n",
    "To get the set of all pairs of real numbers, we would essentially take the product of this set with itself (called the Cartesian product) — $\\mathbb{R}$ x $\\mathbb{R}$, abbreviated as $\\mathbb{R}^2$. This set — $\\mathbb{R}^2$ — contains all pairs of real numbers, so $(1, 3)$ is **contained in** this set. We write this symbolically as $(1, 3) \\in \\mathbb{R}^2$.\n",
    "\n",
    "+ In 2-D space ($\\mathbb{R}^2$), a point is uniquely referred to using two coordinates: $(1, 3) \\in \\mathbb{R}^2$.\n",
    "+ In 3-D space ($\\mathbb{R}^3$), a point is uniquely referred to using three coordinates: $(8, 2, -3) \\in \\mathbb{R}^3$.\n",
    "+ In $n$-dimensional space ($\\mathbb{R}^n$), a point is uniquely referred to using $n$ coordinates.\n",
    "\n",
    "Note that these coordinates of course are isomorphic to our vectors! After all, coordinates are ordered sequences of numbers, just as we define vectors to be ordered sequences of numbers. So, especially in machine learning, we often visualize vectors of length $n$ as points in $n$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-algebra-applications-to-machine-learning\"></a>\n",
    "## Linear Algebra Applications Machine Learning\n",
    "---\n",
    "\n",
    "<a id=\"distance-between-actual-values-and-predicted-values\"></a>\n",
    "### Distance Between Actual Values and Predicted Values\n",
    "We often need to know the difference between predicted values and actual values. That is:\n",
    "$$\\| \\vec{actual} - \\vec{predicted} \\| =\\sqrt{(actual_1 - predicted_1)^2 + (actual_2 - predicted_2)^2 + \\cdots}$$\n",
    "\n",
    "Note that this is just the straight-line distance between the actual point and the predicted point.\n",
    "\n",
    "<a id=\"mean-squared-error\"></a>\n",
    "### Mean Squared Error\n",
    "Often, it's easier to look at the mean of the squared errors. Where $\\hat{\\mathbf{y}}$ is a vector of predicted values (a function of the data matrix $\\mathbf{X}$) and $\\mathbf{y}$ is the actual values:\n",
    "\n",
    "$$MSE = \\frac{1} {n} \\| \\hat{\\mathbf{y}} - \\mathbf{y} \\|^2$$\n",
    "\n",
    "<a id=\"least-squares\"></a>\n",
    "### Least squares\n",
    "Many machine learning models are based on the following form:\n",
    "\n",
    "$$\\min \\| \\hat{\\mathbf{y}} - \\mathbf{y} \\|^2$$\n",
    "\n",
    "The goal is to minimize the distance between model predictions and actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"codealong-examining-the-titanic-dataset\"></a>\n",
    "## Code-Along: Examining the Titanic Data Set\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: Read in the Titanic data and look at a few summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the dimensions of the DataFrame using the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preview data dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the data types of the columns using the `.dtypes` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# What are the column data types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the first five rows of the data using the built-in `.head()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first five rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the built-in  `.value_counts()` function to count the values of each type in the `pclass` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Count the values of the plcass variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull up descriptive statistics for each variable using the built-in `.describe()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pull up descriptive statistics for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing Data Problems\n",
    "\n",
    "- Whenever you get a new data set, the fastest way to find mistakes and inconsistencies is to look at the **descriptive statistics**.\n",
    "  - If anything looks too high or too low relative to your experience, there may be issues with the data collection.\n",
    "- Your data may contain a lot of **missing values** and may need to be cleaned meticulously before they can be combined with other data.\n",
    "  - You can take a quick average or moving average to smooth out the data and combine that to preview your results before you embark on your much longer data-cleaning journey.\n",
    "  - Sometimes filling in missing values with their means or medians will be the best solution for dealing with missing data. Other times, you may want to drop the offending rows or do real imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"descriptive-statistics-fundamentals\"></a>\n",
    "## Descriptive Statistics Fundamentals\n",
    "---\n",
    "\n",
    "- **Objective:** Code summary statistics using NumPy and Pandas: mean, median, mode, max, min, quartile, inter-quartile range, variance, standard deviation, and correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Review of Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of a constant, $k$, $n$ times:\n",
    "$$\\sum_{i=1}^nk$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# k + k + k + k + ... + k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all numbers from 1 up to and including $n$:\n",
    "$$\\sum_{i=1}^ni$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1 + 2 + 3 + ... + n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all $x$ from the first $x$ entry to the $n$th $x$ entry:\n",
    "$$\\sum_{i=1}^nx_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([3, 5, 9, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x_1 + x_2 + x_3 + ... + x_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Along\n",
    "\n",
    "_Optional: Write down the mathematical notation for the following questions:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute the sum of seven 4s using base Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute the sum of seven 4s using NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute the sum of 1 through 10 using base Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Using the titanic.fare column, compute the total fare paid by passengers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"measures-of-central-tendency\"></a>\n",
    "### Measures of Central Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean\n",
    "- Median\n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean\n",
    "The mean is defined as:\n",
    "$$\\bar{x} =\\frac 1n\\sum_{i=1}^nx_i$$\n",
    "\n",
    "It is determined by summing all data points in a population and then dividing the total by the number of points. The resulting number is known as the mean or the average.\n",
    "\n",
    "Be careful — the mean can be highly affected by outliers. For example, the mean of a very large number and some small numbers will be much larger than the \"typical\" small numbers. Earlier, we saw that the mean squared error (MSE) was used to optimize linear regression. Because this mean is highly affected by outliers, the resulting linear regression model is, too.\n",
    "\n",
    "We say the mean is **sensitive** to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median\n",
    "The median refers to the midpoint in a series of numbers. Notice that the median is not affected by outliers, so it more so represents the \"typical\" value in a set.\n",
    "\n",
    "$$ 0,1,2,[3],5,5,1004 $$\n",
    "\n",
    "$$ 1,3,4,[4,5],5,5,7 $$\n",
    "\n",
    "Although the median has many useful properties, the mean is easier to use in optimization algorithms. The median is more often used in analysis than in machine learning algorithms.\n",
    "\n",
    "The median isn't really affected by a few outliers. We say the median is **resistant** to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "The mode of a set of values is the value that occurs most often.\n",
    "A set of values may have more than one mode, or no mode at all.\n",
    "\n",
    "$$1,0,1,5,7,8,9,3,4,1$$ \n",
    "\n",
    "$1$ is the mode, as it occurs the most often (three times)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code-Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the mean of the titanic.fare series using base Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the mean of the titanic.fare series using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the mean of the titanic.fare series using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# What was the median fare paid (using Pandas)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use Pandas to find the most common fare paid on the Titanic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"measures-of-dispersion-standard-deviation-and-variance\"></a>\n",
    "### Measures of Dispersion: Standard Deviation and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in statistics and machine learning, most equations are not identities (e.g. the Pythagorean Theorem and Quadratic Formula). Instead, people invented the equations as a way to measure something. In fact, you can come up with your own measures yourself (and many people have ...).\n",
    "\n",
    "#### Variance intuition\n",
    "\n",
    "1. **Range.** The first way you may think of to measure dispersion is just the range of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.array([13, 15, 10, 80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Mean Absolute Deviation.** Second, it might make sense to just take the average distance from the mean:\n",
    "\n",
    "$$\\text{MAD}(x) = \\frac{\\sum_{i=1}^{N}{|x_i - \\bar{x}|}}{N}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what about these -- intutively, should one of these have a larger dispersion?\n",
    "ages1 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 5])\n",
    "ages2 = np.array([1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Variance.** And so, if we square the distances from the mean we might get a more natural measure of dispersion:\n",
    "\n",
    "$$\\text{VAR}(x) = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Standard Deviation.** Unfortunately, variance is not in the original units!\n",
    "\n",
    "Standard deviation (SD, $σ$ for population standard deviation, or $s$ for sample standard deviation) is a measure that is used to quantify the amount of variation or dispersion from the mean of a set of data values. A low standard deviation means that most of the numbers are close to the average. A high standard deviation means that the numbers are spread out.\n",
    "\n",
    "Standard deviation is the square root of variance:\n",
    "\n",
    "$$\\text{variance} = \\sigma^2 = \\frac {\\sum{(x_i - \\bar{x})^2}} {n}$$\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac {\\sum{(x_i - \\bar{x})^2}} {n}}$$\n",
    "\n",
    "> **Standard deviation** is often used because it is in the same units as the original data! By glancing at the standard deviation, we can immediately estimate how \"typical\" a data point might be by how many standard deviations it is from the mean. Furthermore, standard deviation is the only value that makes sense to visually draw alongside the original data.\n",
    "\n",
    "> **Variance** is often used for efficiency in computations. The square root in the SD always increases with the function to which it is applied. So, removing it can simplify calculations (e.g., taking derivatives), particularly if we are using the variance for tasks such as optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### One more important point: Bias, and Population vs. Sample\n",
    "- **Bias:** The difference between the estimator's expected value and the true value of the parameter being estimated.\n",
    "\n",
    "So far, we have been dividing by $n$, the number of points. If we know that the number of points constitutes the entire population, then this is indeed the variance of the population -- it is **unbiased**.\n",
    "\n",
    "However, if our points consist of a sample of the population, this will underestimate the actual variance of the population -- dividing by $n$ would result in a **biased estimate**.\n",
    "\n",
    "We can correct for this easily by dividing by $n-1$ instead of $n$ (the math is beyond the scope of this class). For example:\n",
    "\n",
    "$$\\text{Unbiased sample variance} = s^2 = \\frac {\\sum{(x_i - \\bar{x})^2}} {n-1}.$$\n",
    "\n",
    "**When using numpy and pandas, always double-check whether the statistics are biased or unbiased!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That can be a lot to take in, so let's break it down in Python.**\n",
    "\n",
    "#### Assign the first 5 rows of titanic age data to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That can be a lot to take in, so let's break it down in Python.**\n",
    "\n",
    "#### Assign the first 5 rows of titanic age data to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Take the first five rows of titanic age data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the variance by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate variance by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the variance and the standard deviation using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Verify with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"math-review\"></a>\n",
    "### Related to Variance: Distance\n",
    "\n",
    "#### How Do We Measure Distance?\n",
    "\n",
    "One method is to take the difference between two points:\n",
    "\n",
    "$$X_2 - X_1$$\n",
    "\n",
    "However, this can be inconvenient because of negative numbers.\n",
    "\n",
    "We often use the following square root trick to deal with negative numbers. Note this is equivalent to the absolute value (if the points are 1-D):\n",
    "\n",
    "$$\\sqrt{(X_2-X_1)^2} = | X_2 - X_1 |$$\n",
    "\n",
    "#### What About Distance in Multiple Dimensions?\n",
    "\n",
    "We can turn to the Pythagorean theorem.\n",
    "\n",
    "$$a^2 + b^2 = c^2$$\n",
    "\n",
    "To find the distance along a diagonal, it is sufficient to measure one dimension at a time:\n",
    "\n",
    "$$\\sqrt{a^2 + b^2} = c$$\n",
    "\n",
    "More generally, we can write this as the norm (You'll see this in machine learning papers):\n",
    "\n",
    "$$\\|X\\|_2 = \\sqrt{\\sum{x_i^2}} = c$$\n",
    "\n",
    "What if we want to work with points rather than distances? For points $\\vec{x}: (x_1, x_1)$ and $\\vec{y}: (y_1, y_2)$ we can write:\n",
    "\n",
    "$$\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} = c$$\n",
    "or\n",
    "$$\\sqrt{\\sum{(x_i - y_i)^2}} = c$$\n",
    "or\n",
    "$$\\| \\vec{x} - \\vec{y} \\| = c$$\n",
    "\n",
    "> You may be more familiar with defining points as $(x, y)$ rather than $(x_1, x_2)$. However, in machine learning it is much more convenient to define each coordinate using the same base letter with a different subscript. This allows us to easily represent a 100-dimensional point, e.g., $(x_1, x_2, ..., x_{100})$. If we use the grade school method, we would soon run out of letters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"our-first-model\"></a>\n",
    "## Our First Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will make a **mathematical model** of data. When we say **model**, we mean it in the same sense that a map is a **model** of the real world. Google Maps can get us to that restaurant without getting lost, but it can't tell us where each individual pothole is. This is good enough.\n",
    "\n",
    "<img src=\"http://www.azquotes.com/picture-quotes/quote-all-models-are-wrong-but-some-are-useful-george-e-p-box-53-42-27.jpg\">\n",
    "\n",
    "In data science, we might take a rich, complex person and model that person solely as a two-dimensional vector: _(high school GPA, SAT score)_. For example: $(3.7, 1450)$, $(3.0, 1200)$, and $(2.8, 1050)$. This model of a complex person obviously fails to account for many things. However, if we primarily care about modeling college success, it might provide valuable insight.\n",
    "\n",
    "Now that we have superficially modeled a complex person, we might determine a formula that evaluates college GPA. For example a student who did well in high school and had a high SAT score will likely do better in college. There are other factors at play, but _**this is just a model**_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Null Model\n",
    "Let's make our first model from scratch. We'll use it predict the `fare` column in the Titanic data. So what data will we use? Actually, none.\n",
    "\n",
    "The simplest model we can build is an estimation of the mean, median, or most common value. If we have no feature matrix and only an outcome, this is the best approach to make a prediction using only empirical data. \n",
    "\n",
    "This seems silly, but we'll actually use it all the time to create a baseline of how well we do with no data and determine whether or not our more sophisticated models make an improvement. This will be called the **null model**, since it uses no information from our $x$-data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the `fare` column from the Titanic data and store it in variable `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the fare column from the Titanic data and store it as y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictions `y_pred` (in this case just the mean of `y`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Stored predictions in y_pred:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the average squared distance between each prediction and its actual value:\n",
    "\n",
    "This is known as the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Squared error is hard to read; let's look at mean squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the root mean squared error (RMSE), the square root of the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-the-normal-distribution\"></a>\n",
    "### What is the Normal Distribution?\n",
    "- A normal distribution is often a key assumption to many models.\n",
    "  - In practice, if the normal distribution assumption is not met, it's not the end of the world. Your model is just less efficient in most cases.\n",
    "\n",
    "- The normal distribution is **completely summarized by its mean and standard deviation**.\n",
    "\n",
    "- The **mean** controls its **center**.\n",
    "\n",
    "- The **standard deviation** controls how **spread out** it is.\n",
    "\n",
    "- Normal distributions are **symmetric, bell-shaped curves**.\n",
    "\n",
    "![normal distribution](assets/images/normal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we care about normal distributions?\n",
    "\n",
    "- They often show up in nature.\n",
    "- Aggregated processes tend to distribute normally, regardless of their underlying distribution (**Central Limit Theorem**!!!)\n",
    "- They offer effective simplification that makes it easy to make approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a histogram of 1,000 samples from a random normal distribution:\n",
    "\n",
    "The `np.random.randn(numsamples)` function will draw from a random normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "- To plot a histogram, pass a NumPy array with 1000 samples as the only parameter to `plt.hist()`.\n",
    "- Change the number of bins using the keyword argument `bins`, e.g. `plt.hist(mydata, bins=50)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c984e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of several random normal samples from NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"skewness\"></a>\n",
    "###  Skewness\n",
    "- Skewness is a measure of the asymmetry of the distribution of a random variable about its mean.\n",
    "- Skewness can be positive or negative, or even undefined.\n",
    "- Notice that the mean, median, and mode are the same when there is no skew.\n",
    "\n",
    "![skewness](assets/images/skewness---mean-median-mode.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a lognormal distribution generated with NumPy.\n",
    "\n",
    "Take 1,000 samples using `np.random.lognormal(size=numsamples)` and plot them on a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a lognormal distribution generated with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Real World Application - When mindfullness beats complexity\n",
    "- Skewness is surprisingly important.\n",
    "- Most algorithms implicitly use the mean by default when making approximations.\n",
    "- If you know your data is heavily skewed, you may have to either transform your data or set your algorithms to work with the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kurtosis\"></a>\n",
    "### Kurtosis\n",
    "- Kurtosis is a measure of whether the data are peaked or flat, relative to a normal distribution.\n",
    "- Data sets with high kurtosis tend to have a distinct peak near the mean, decline rather rapidly, and have heavy tails. \n",
    "\n",
    "![kurtosis](assets/images/kurtosis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Real-World Application: Risk Analysis\n",
    "- Long-tailed distributions with high kurtosis elude intuition; we naturally think the event is too improbable to pay attention to.\n",
    "- It's often the case that there is a large cost associated with a low-probability event, as is the case with hurricane damage.\n",
    "- It's unlikely you will get hit by a Category 5 hurricane, but when you do, the damage will be catastrophic.\n",
    "- Pay attention to what happens at the tails and whether this influences the problem at hand.\n",
    "- In these cases, understanding the costs may be more important than understanding the risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"math-review\"></a>\n",
    "## BONUS: Covariance and Correlation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"covariance\"></a>\n",
    "### Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is a measure of the joint variability between two random variables.\n",
    "\n",
    "You can think of this as a measure of linear association. If you have a variance of Y and a variance of X, the covariance is the amount of variance they share.\n",
    "\n",
    "$$\\text{Cov}(X, Y) = \\frac {\\sum{(x_i - \\bar{X})(y_i - \\bar{Y})}} {n}$$\n",
    "\n",
    "#### The math can be a bit intimidating, but I show it only to ask these two questions:\n",
    "\n",
    "* When will covariance be positive?\n",
    "* How will outliers affect covariance?\n",
    "\n",
    "**Covariance Expressed Using Matrix Notation**\n",
    "\n",
    "$$\\text{Cov}(\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}[(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}])]$$\n",
    "\n",
    "**A Useful Special Case (Used Below)**\n",
    "\n",
    "$$\\text{Cov}(X, X) = \\frac {\\sum{(x_i - \\bar{X})^2}} {n} = \\text{Var}(X) = \\sigma_X^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"correlation\"></a>\n",
    "### Correlation\n",
    "\n",
    "While covariance is a useful measure, it can be difficult to compare covariances, as they are not standardized. \n",
    "\n",
    "Instead we can use the correlation, which measures the same effect but reports it as a range from -1 to 1. 1 represents perfect covariance and correlation, 0 represents no correlation, and -1 one represents perfect inverse correlation.\n",
    "\n",
    "$$\\text{Corr}(X,Y) = \\frac {\\text{Cov}(X,Y)} {\\sigma_X\\sigma_Y} = \\frac {\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]} {\\sigma_X\\sigma_Y}$$\n",
    "\n",
    "Note that the variance is always positive, making the denominator positive. So, the sign of the covariance between $X$ and $Y$ is the same as the sign of their correlation! \n",
    "\n",
    "The following visual examples better illustrate how correlation refers to how $X$ and $Y$ change together. Notice that a correlation number by itself is not always indicative of the relationship between the variables — always try to supplement 2-D correlation with a visual!\n",
    "\n",
    "![](assets/images/correlation_examples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-variance-covariance-matrix\"></a>\n",
    "### The Variance-Covariance Matrix\n",
    "\n",
    "If you have many $x$-variables, it's common to organize covariances into a **var-covar matrix** (sometimes just called a **covariance matrix**).\n",
    "\n",
    "Given $n$ features from $X_1$ to $X_n$, the variance-covariance matrix looks like this (recall that $cov(X, X) = var(X)$):\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma} = \n",
    "\\left[ \\begin{array}{c}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1,X_2) & \\cdots & \\text{Cov}(X_1,X_n)  \\\\\n",
    "\\text{Cov}(X_2,X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2,X_n)  \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_n,X_1) & \\text{Cov}(X_n,X_2) & \\cdots & \\text{Var}(X_n)\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "But... still not useful right? We can't read covariances easily. So it's also common to have a **correlation matrix**:\n",
    "\n",
    "$$\n",
    "\\mathbf{R} = \n",
    "\\begin{bmatrix}\n",
    "1 & \\text{Corr}(X_1, X_2) & \\cdots & \\text{Corr}(X_1, X_n) \\\\\n",
    "\\text{Corr}(X_2, X_1) & 1 & \\cdots & \\text{Corr}(X_2, X_n) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Corr}(X_n, X_1) & \\text{Corr}(X_n, X_2) & \\cdots & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the var-covar matrix using the DataFrame's built-in `.cov()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>0.236772</td>\n",
       "      <td>-0.137703</td>\n",
       "      <td>-0.551296</td>\n",
       "      <td>-0.018954</td>\n",
       "      <td>0.032017</td>\n",
       "      <td>6.221787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pclass</th>\n",
       "      <td>-0.137703</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>-4.496004</td>\n",
       "      <td>0.076599</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>-22.830196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.551296</td>\n",
       "      <td>-4.496004</td>\n",
       "      <td>211.019125</td>\n",
       "      <td>-4.163334</td>\n",
       "      <td>-2.344191</td>\n",
       "      <td>73.849030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sibsp</th>\n",
       "      <td>-0.018954</td>\n",
       "      <td>0.076599</td>\n",
       "      <td>-4.163334</td>\n",
       "      <td>1.216043</td>\n",
       "      <td>0.368739</td>\n",
       "      <td>8.748734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parch</th>\n",
       "      <td>0.032017</td>\n",
       "      <td>0.012429</td>\n",
       "      <td>-2.344191</td>\n",
       "      <td>0.368739</td>\n",
       "      <td>0.649728</td>\n",
       "      <td>8.661052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare</th>\n",
       "      <td>6.221787</td>\n",
       "      <td>-22.830196</td>\n",
       "      <td>73.849030</td>\n",
       "      <td>8.748734</td>\n",
       "      <td>8.661052</td>\n",
       "      <td>2469.436846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          survived     pclass         age     sibsp     parch         fare\n",
       "survived  0.236772  -0.137703   -0.551296 -0.018954  0.032017     6.221787\n",
       "pclass   -0.137703   0.699015   -4.496004  0.076599  0.012429   -22.830196\n",
       "age      -0.551296  -4.496004  211.019125 -4.163334 -2.344191    73.849030\n",
       "sibsp    -0.018954   0.076599   -4.163334  1.216043  0.368739     8.748734\n",
       "parch     0.032017   0.012429   -2.344191  0.368739  0.649728     8.661052\n",
       "fare      6.221787 -22.830196   73.849030  8.748734  8.661052  2469.436846"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "titanic.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the correlation matrix using the DataFrame's built-in `.corr()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.257307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pclass</th>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.369226</td>\n",
       "      <td>0.083081</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>-0.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.369226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.308247</td>\n",
       "      <td>-0.189119</td>\n",
       "      <td>0.096067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sibsp</th>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.083081</td>\n",
       "      <td>-0.308247</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>0.159651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parch</th>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>-0.189119</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare</th>\n",
       "      <td>0.257307</td>\n",
       "      <td>-0.549500</td>\n",
       "      <td>0.096067</td>\n",
       "      <td>0.159651</td>\n",
       "      <td>0.216225</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          survived    pclass       age     sibsp     parch      fare\n",
       "survived  1.000000 -0.338481 -0.077221 -0.035322  0.081629  0.257307\n",
       "pclass   -0.338481  1.000000 -0.369226  0.083081  0.018443 -0.549500\n",
       "age      -0.077221 -0.369226  1.000000 -0.308247 -0.189119  0.096067\n",
       "sibsp    -0.035322  0.083081 -0.308247  1.000000  0.414838  0.159651\n",
       "parch     0.081629  0.018443 -0.189119  0.414838  1.000000  0.216225\n",
       "fare      0.257307 -0.549500  0.096067  0.159651  0.216225  1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer:\n",
    "titanic.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"topic-review\"></a>\n",
    "## Lesson Review\n",
    "---\n",
    "\n",
    "- We covered several different types of summary statistics, what are they?\n",
    "- We covered three different types of visualizations, which ones?\n",
    "- Describe bias and variance and why they are important.\n",
    "- What are some important characteristics of distributions?\n",
    "\n",
    "**Any further questions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
